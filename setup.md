# Setup del Proyecto - Jupyter Notebooks + Python + LangChain

## üìã Resumen
Configuraci√≥n completa del entorno para ejecutar `chatmodel.ipynb` usando:
- **Python 3.11.2** (entorno virtual)
- **Jupyter Notebooks** (en VS Code)
- **Groq API** (LLM gratuito)
- **HuggingFace Embeddings** (gratuito, local)

---

## 1. Verificar Python Instalado
```bash
python3 --version
# Output esperado: Python 3.11.2
```

---

## 2. Instalar Extensi√≥n Jupyter en VS Code
- Abrir VS Code
- Extensions (Ctrl+Shift+X)
- Buscar "Jupyter"
- Instalar extensi√≥n oficial de Microsoft

---

## 3. Crear Entorno Virtual
```bash
python3 -m venv .venv
```
**¬øQu√© hace?** Crea carpeta `.venv` con Python aislado para este proyecto.

---

## 4. Instalar Dependencias Base

### Jupyter
```bash
.venv/bin/pip install jupyter
```
**Qu√© instala:**
- `jupyter`: Framework completo para notebooks
- `ipykernel`: Motor que ejecuta c√≥digo Python en las celdas

### LangChain Core
```bash
.venv/bin/pip install langchain langchain-core langchain-community
```
**Qu√© instala:**
- `langchain`: Framework principal para LLMs
- `langchain-core`: N√∫cleo de LangChain (prompts, parsers, etc.)
- `langchain-community`: Herramientas comunitarias

### Integraciones LLM
```bash
.venv/bin/pip install langchain-groq langchain-openai
```
**Qu√© instala:**
- `langchain-groq`: Integraci√≥n con Groq (LLM gratuito)
- `langchain-openai`: Clases de OpenAI (usado en el notebook original)

### Base de Datos Vectorial
```bash
.venv/bin/pip install langchain-chroma
```
**Qu√© instala:**
- `langchain-chroma`: Base de datos vectorial para b√∫squedas sem√°nticas

### Embeddings Locales (HuggingFace)
```bash
.venv/bin/pip install sentence-transformers langchain-huggingface
```
**Qu√© instala:**
- `sentence-transformers`: Modelos de embeddings de HuggingFace
- `langchain-huggingface`: Integraci√≥n LangChain + HuggingFace
- **Incluye:** PyTorch, transformers, scikit-learn, scipy (~3GB total)

### An√°lisis de Datos
```bash
.venv/bin/pip install pandas
```
**Qu√© instala:**
- `pandas`: An√°lisis y manipulaci√≥n de datos (DataFrames)

### Procesamiento de PDFs (para RAG)
```bash
.venv/bin/pip install pypdf
```
**Qu√© instala:**
- `pypdf`: Lector de archivos PDF para ingesta de documentos
- **Usado en:** raglangchain.ipynb (cargar PDFs como contexto)

### LangSmith (para prompts p√∫blicos y observabilidad)
**Nota:** `langsmith` ya se instal√≥ en la secci√≥n de Utilidades.

**Uso adicional en RAG:**
- Descargar prompts p√∫blicos del hub con `client.pull_prompt()`
- Ejemplo: `hub_client.pull_prompt("rlm/rag-prompt")`
- **Usado en:** raglangchain.ipynb

### Utilidades
```bash
.venv/bin/pip install python-dotenv langsmith
```
**Qu√© instala:**
- `python-dotenv`: Carga variables desde archivo `.env`
- `langsmith`: Cliente de LangSmith (observabilidad, opcional)

---

## 5. Crear `.gitignore`
```bash
# Crear archivo .gitignore en la ra√≠z del proyecto
```

**Contenido:**
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python

# Entorno virtual
.venv/
venv/
ENV/

# Jupyter
.ipynb_checkpoints/

# Variables de entorno
.env

# VS Code
.vscode/
```

---

## 6. Obtener Groq API Key

1. Ir a https://console.groq.com/
2. Crear cuenta (gratis, sin tarjeta)
3. Click "API Keys" ‚Üí "Create API Key"
4. Copiar la key (empieza con `gsk_...`)

---

## 7. Crear Archivo `.env`

```bash
# Crear .env en la ra√≠z del proyecto
```

**Contenido:**
```bash
# Groq API Key (obtenida en paso 6)
GROQ_API_KEY=gsk_tu_key_aqui

# Modelo a usar (actualizado a llama-3.1-8b-instant)
OPENAI_MODEL=llama-3.1-8b-instant
```

**Nota:** El `.gitignore` ya est√° configurado para NO subir este archivo a Git.

---

## 8. Modificar `chatmodel.ipynb`

### Cambio 1: Celda 5 (imports)
**Antes:**
```python
from langchain.memory import ChatMessageHistory
from langchain_openai import OpenAIEmbeddings
```

**Despu√©s:**
```python
from langchain_core.chat_history import InMemoryChatMessageHistory
# from langchain_openai import OpenAIEmbeddings  # ‚ùå OpenAI (de pago)
from langchain_huggingface import HuggingFaceEmbeddings  # ‚úÖ HuggingFace (gratis)
```

### Cambio 2: Celda 7 (configuraci√≥n LLM)
**Antes:**
```python
llm = ChatOpenAI(model=llm_model, temperature=0.1)
# llm = ChatGroq(model=llm_model, temperature=0.1)
```

**Despu√©s:**
```python
# llm = ChatOpenAI(model=llm_model, temperature=0.1)
llm = ChatGroq(model=llm_model, temperature=0.1)
```

### Cambio 3: Celda 21 (embeddings)
**Antes:**
```python
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),  # ‚ùå Requiere OPENAI_API_KEY
    Chroma,
    k=1,
)
```

**Despu√©s:**
```python
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    # OpenAIEmbeddings(),  # ‚ùå Antiguo: requiere OPENAI_API_KEY
    HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"),  # ‚úÖ Nuevo: gratis
    Chroma,
    k=1,
)
```

---

## 9. Seleccionar Kernel en VS Code

1. Abrir `chatmodel.ipynb` en VS Code
2. Click "Select Kernel" (arriba derecha)
3. Elegir "Python Environments" ‚Üí `.venv/bin/python`
4. Verificar que aparezcan botones ‚ñ∂Ô∏è Play en cada celda

---

## 10. Ejecutar el Notebook

### Orden de ejecuci√≥n:
1. **Reiniciar kernel** si estaba abierto (bot√≥n "Restart")
2. **Ejecutar celdas en orden:**
   - Celda 2: Imports b√°sicos
   - Celda 3: `load_dotenv()` ‚Üí debe mostrar `True`
   - Celda 4: SALTAR (comentada con `%pip install`)
   - Celda 5: Imports LangChain
   - Celda 7: Primera llamada a Groq (chiste)
   - Continuar en orden...

### Primera ejecuci√≥n de Celda 21:
- Descargar√° modelo `all-MiniLM-L6-v2` (~90MB)
- Tardar√° unos segundos
- Ejecuciones siguientes usar√°n cach√© local

---

## üì¶ Dependencias Instaladas (Resumen)

| Paquete | Versi√≥n | Prop√≥sito |
|---------|---------|-----------|
| `jupyter` | Latest | Framework notebooks |
| `langchain` | Latest | Framework LLM |
| `langchain-core` | Latest | N√∫cleo LangChain |
| `langchain-groq` | Latest | Integraci√≥n Groq |
| `langchain-huggingface` | Latest | Integraci√≥n HuggingFace |
| `langchain-chroma` | Latest | Base de datos vectorial |
| `sentence-transformers` | Latest | Embeddings locales |
| `pandas` | Latest | An√°lisis de datos |
| `python-dotenv` | Latest | Variables de entorno |

---

## üîß Troubleshooting

### Error: "No module named 'langchain_huggingface'"
**Soluci√≥n:**
```bash
.venv/bin/pip install langchain-huggingface
```

### Error: "Model llama-3.1-70b-versatile has been decommissioned"
**Soluci√≥n:** Actualizar `.env`
```bash
OPENAI_MODEL=llama-3.1-8b-instant
```

### Error: "OPENAI_API_KEY environment variable not set"
**Soluci√≥n:** Usar `HuggingFaceEmbeddings` en vez de `OpenAIEmbeddings` (ya corregido en paso 8)

### Kernel no aparece en VS Code
**Soluci√≥n:**
1. Cerrar VS Code completamente
2. Reabrir proyecto
3. Seleccionar kernel nuevamente

---

## ‚úÖ Verificaci√≥n Final

Lista de chequeo antes de ejecutar el notebook:

- [ ] Python 3.11.2 instalado
- [ ] Extensi√≥n Jupyter en VS Code
- [ ] Entorno virtual `.venv` creado
- [ ] Todas las dependencias instaladas
- [ ] Archivo `.env` creado con `GROQ_API_KEY` y `OPENAI_MODEL=llama-3.1-8b-instant`
- [ ] Archivo `.gitignore` creado
- [ ] `chatmodel.ipynb` modificado (3 cambios)
- [ ] Kernel `.venv/bin/python` seleccionado en VS Code

---

## üéØ Pr√≥ximos Pasos

1. Ejecutar notebook celda por celda
2. Revisar `notebooks/chatmodel.md` para entender cada celda
3. Revisar `notebooks/README.md` para gu√≠a de aprendizaje
4. Experimentar modificando prompts y par√°metros
