{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cb0be8",
   "metadata": {},
   "source": [
    "# Prompting and basic Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c79b73",
   "metadata": {},
   "source": [
    "Importing necessary libraries and installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afca013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pprint\n",
    "from typing import List\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d58cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-openai langchain-groq langchain-community langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d57f27",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langchain_groq import ChatGroq\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.chat_history import InMemoryChatMessageHistory\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelector\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n# from langchain_openai import OpenAIEmbeddings  # ❌ Requiere OPENAI_API_KEY (de pago)\nfrom langchain_huggingface import HuggingFaceEmbeddings  # ✅ Gratis, local\nfrom langchain_chroma import Chroma\nfrom pydantic import BaseModel, Field"
  },
  {
   "cell_type": "markdown",
   "id": "f7b61bac",
   "metadata": {},
   "source": [
    "## A simple LLM-based Chat\n",
    "\n",
    "For Groq, you need to get first an account and API KEY at https://groq.com/ \n",
    "\n",
    "The API KEY should go to the env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9942cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = os.environ[\"OPENAI_MODEL\"]\n",
    "# llm_model = \"moonshotai/kimi-k2-instruct\"\n",
    "print(llm_model)\n",
    "\n",
    "# llm = ChatOpenAI(model=llm_model, temperature=0.1)\n",
    "llm = ChatGroq(model=llm_model, temperature=0.1)\n",
    "\n",
    "response = llm.invoke(\"Tell me a joke about data scientists\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be39da",
   "metadata": {},
   "source": [
    "Chat models are based on roles and messages. Let's do our first chain with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm # Chaining the prompt and the llm call\n",
    "\n",
    "ai_msg = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1baea",
   "metadata": {},
   "source": [
    "This is a another typical kind of chain. Here, we use a *zero-shot* mode, as the LLM answers with its own (pretrained) knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an experienced software architect that assists a novice developer \n",
    "to design a system. \"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", SYSTEM_PROMPT),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    ")\n",
    "\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "query = \"What are the pros and cons of the Proxy design pattern?\"\n",
    "result = qa_chain.invoke(input=query)\n",
    "# print(result.content)\n",
    "\n",
    "display(Markdown(result.content)) # Result in Markdown format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071cf33",
   "metadata": {},
   "source": [
    "## Handling Memory (as part of a chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa52eff",
   "metadata": {},
   "source": [
    "The memory is a list of previous (pairs of) messages between the human and the assistant, which provide *context* for the next iteraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTUALIZED_PROMPT = \"\"\"Given a chat history and the latest developer's question\n",
    "    which might reference context in the chat history, formulate a standalone question\n",
    "    that can be understood without the chat history. Do NOT answer the question,\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualized_qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", CONTEXTUALIZED_PROMPT),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# This is a possible chain to keep (and compress) past interactions\n",
    "# It's a form of rewriting\n",
    "contextualized_qa_chain = contextualized_qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# A buffer to store messages from user and assistant\n",
    "chat_history = InMemoryChatMessageHistory()\n",
    "chat_history.add_user_message(query)\n",
    "chat_history.add_ai_message(result)\n",
    "\n",
    "\n",
    "query = \"Can I combine the pattern with other patterns?\"\n",
    "ai_msg = contextualized_qa_chain.invoke(\n",
    "    {\n",
    "        'question': query, \n",
    "        'chat_history': chat_history.messages\n",
    "    }\n",
    ")\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09ce73",
   "metadata": {},
   "source": [
    "Let's use the (compressed) history to answer the new question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a198b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualized_question(input: dict):\n",
    "        if input.get(\"chat_history\"):\n",
    "            return contextualized_qa_chain\n",
    "        else:\n",
    "            return input[\"question\"]\n",
    "\n",
    "# A new QA chain that reuses the previous chain\n",
    "qa_chain_with_memory = (\n",
    "         RunnablePassthrough.assign(\n",
    "            context=contextualized_question | qa_prompt | llm\n",
    "        )\n",
    "    )\n",
    "\n",
    "result = qa_chain_with_memory.invoke(\n",
    "    {\n",
    "        'question': query,  \n",
    "        'chat_history': chat_history.messages\n",
    "    }\n",
    ")\n",
    "\n",
    "display(Markdown(result['context'].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e91488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the whole result looks like\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5df46e",
   "metadata": {},
   "source": [
    "## Handling Few-shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=['input'],\n",
    "    template=\"\"\"Return the antonym of the input given along with an explanation.\n",
    "    Input: {input}\n",
    "    Output:\n",
    "    Explanation:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Zero-shot chain\n",
    "zero_shot_chain = zero_shot_prompt | llm\n",
    "\n",
    "query = 'I am very sad but still have hope'\n",
    "result = zero_shot_chain.invoke(input=query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7305a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of a task for creating antonyms.\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839c88c",
   "metadata": {},
   "outputs": [],
   "source": "example_selector = SemanticSimilarityExampleSelector.from_examples(\n    # The list of examples available to select from.\n    examples,\n    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n    # OpenAIEmbeddings(),  # ❌ Antiguo: requiere OPENAI_API_KEY\n    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),  # ✅ Nuevo: gratis, local\n    Chroma, # The database to store the examples with their embeddings\n    # The number of examples to produce.\n    k=1,\n)\n\nsimilar_prompt = FewShotPromptTemplate(\n    # We provide an ExampleSelector instead of examples.\n    example_selector=example_selector,\n    example_prompt=example_prompt,\n    prefix=\"Return the antonym of the given input along with an explanation. \\n\\nExample(s):\",\n    suffix=\"Input: {adjective}\\nOutput:\",\n    input_variables=[\"adjective\"],\n)\n\nprint(similar_prompt.format(adjective=\"rainy\"))\n"
  },
  {
   "cell_type": "markdown",
   "id": "def8f9b4",
   "metadata": {},
   "source": [
    "And let's use the new prompt in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ec565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot\n",
    "few_shot_chain = similar_prompt | llm\n",
    "\n",
    "query = 'rainy' # 'I am very sad but still have hope'\n",
    "print(similar_prompt.format(adjective=query))\n",
    "print()\n",
    "\n",
    "result = few_shot_chain.invoke(input=query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed851dcd",
   "metadata": {},
   "source": [
    "We can ask an LLM to generate its response as a JSON object, using the Pydantic framework. \n",
    "\n",
    "For example, we can format the antonym output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ca2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormattedAntonym(BaseModel):\n",
    "    antonym: str = Field(description=\"An antonym for the input word or phrase.\")\n",
    "    explanation: str = Field(description=\"A short explanation of how the antonym was generated\")\n",
    "    additional_clarifications: List[str] = Field(description=\"A list of questions that the LLM needs to clarify in order to ...\", default=[])\n",
    "\n",
    "\n",
    "llm_with_structure = llm.with_structured_output(FormattedAntonym)\n",
    "\n",
    "few_shot_chain1 = similar_prompt | llm_with_structure\n",
    "result = few_shot_chain1.invoke(input=query)\n",
    "result # The Pydantic (object) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result.model_dump_json(indent=2))\n",
    "pprint.pprint(result.model_dump()) # This is a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca44fb",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}