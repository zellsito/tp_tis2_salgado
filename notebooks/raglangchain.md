# Aprendizaje: raglangchain.ipynb

## üìã Resumen Ejecutivo

**Notebook:** raglangchain.ipynb (254KB adaptado)
**Tema principal:** Retrieval Augmented Generation (RAG) - Sistema que combina b√∫squeda sem√°ntica con LLMs
**Estado:** ‚úÖ COMPLETADO - 100% funcional con embeddings locales
**Fecha:** 2025-11-07

### üéØ Objetivo del Notebook
Aprender RAG (Retrieval Augmented Generation), una t√©cnica que combina:
1. **B√∫squeda sem√°ntica** (recuperar documentos relevantes de una base de datos vectorial)
2. **LLM** (generar respuestas basadas en los documentos recuperados)

RAG permite que un LLM responda preguntas con informaci√≥n actualizada y espec√≠fica de tus documentos, sin necesidad de re-entrenar el modelo.

---

## üîß Errores Encontrados y Corregidos

### 1. Imports Deprecados de LangChain 1.0+
```python
# ‚ùå ANTES (LangChain 0.x)
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.vectorstores.chroma import Chroma

# ‚úÖ DESPU√âS (LangChain 1.0+)
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_chroma import Chroma
```

### 2. Hub de LangChain
```python
# ‚ùå ANTES
from langchain import hub
rag_prompt = hub.pull("rlm/rag-prompt")

# ‚úÖ DESPU√âS
from langsmith import Client as LangSmithClient
hub_client = LangSmithClient()
rag_prompt = hub_client.pull_prompt("rlm/rag-prompt")
```

### 3. Embeddings y LLM (OpenAI ‚Üí Local/Gratis)
```python
# ‚ùå ANTES
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
embeddings = OpenAIEmbeddings()
llm = ChatOpenAI()

# ‚úÖ DESPU√âS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.1)
```

### 4. Re-ranking (ContextualCompressionRetriever ‚Üí Manual)
```python
# ‚ùå ANTES (No disponible en LangChain 1.0+)
from langchain.retrievers import ContextualCompressionRetriever
compression_retriever = ContextualCompressionRetriever(...)

# ‚úÖ DESPU√âS (Implementaci√≥n manual)
from sentence_transformers import CrossEncoder

cross_encoder_model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

def rerank_documents(query: str, documents: list, top_n: int = 3):
    pairs = [[query, doc.page_content] for doc in documents]
    scores = cross_encoder_model.predict(pairs)
    scored_docs = list(zip(documents, scores))
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, score in scored_docs[:top_n]]
```

### 5. RAGxplorer (Instalaci√≥n y Configuraci√≥n)
```bash
# ‚ùå pip install ragexplorer  # No disponible en PyPI
# ‚úÖ Instalar desde GitHub
pip install git+https://github.com/gabrielchua/RAGxplorer.git

# Parches necesarios:
# 1. ragxplorer/rag.py - actualizar imports de text_splitter
# 2. ragxplorer/projections.py - convertir listas a numpy arrays
```

```python
# ‚úÖ Uso con embeddings locales
from ragxplorer import RAGxplorer

client = RAGxplorer(embedding_model="all-MiniLM-L6-v2")
client.load_pdf(document_path="./data/Understanding_Climate_Change.pdf",
                chunk_size=1000, chunk_overlap=100)

# Usar m√©todo "naive" en lugar de "HyDE" (bug con embeddings locales)
client.visualize_query(query="What is climate change?",
                       retrieval_method="naive", top_k=6)
```

---

## üìö Conceptos Clave

### 1. **RAG (Retrieval Augmented Generation)**
Combina dos componentes:
- **Retrieval:** Busca documentos relevantes en una base de datos vectorial
- **Generation:** LLM genera respuesta usando los documentos como contexto

**Flujo:**
```
User Query ‚Üí Vector Search ‚Üí Top K Docs ‚Üí (Optional Re-ranking) ‚Üí LLM ‚Üí Answer
```

### 2. **Embeddings**
Representaci√≥n vectorial del texto que captura su significado sem√°ntico.
- Usamos `all-MiniLM-L6-v2` (384 dimensiones, local, gratis)
- Textos similares tienen embeddings cercanos en el espacio vectorial

### 3. **Vector Store (ChromaDB)**
Base de datos que almacena documentos como vectores.
- Permite b√∫squeda por similitud sem√°ntica
- En este notebook: ChromaDB con persistencia en memoria

### 4. **Chunking (Divisi√≥n de Documentos)**
Dividir documentos largos en fragmentos peque√±os para:
- Mejorar la relevancia de la b√∫squeda
- Caber en el contexto del LLM
- `chunk_size=1000`, `chunk_overlap=100`

### 5. **Re-ranking con Cross-Encoder**
Mejora la calidad de los documentos recuperados:
- **Bi-encoder** (embeddings): R√°pido, busca en millones de docs
- **Cross-encoder**: Lento pero m√°s preciso, re-rankea top K

**Diferencia:**
- Bi-encoder: Encode query y docs por separado, compara vectores
- Cross-encoder: Encode (query + doc) juntos, score de relevancia directo

### 6. **Prompt Template para RAG**
```
You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.

Context: {context}
Question: {question}

Answer:
```

---

## üìù Estructura del Notebook

### Parte 1: Setup y B√∫squeda Sem√°ntica con Movies
- Cargar dataset de pel√≠culas (JSON)
- Crear documentos con metadata (g√©nero, fecha, idioma)
- Vector store en memoria (`InMemoryVectorStore`)
- B√∫squeda sem√°ntica b√°sica con scores
- Crear retriever con filtros

**Conceptos aprendidos:**
- `similarity_search()` vs `similarity_search_with_score()`
- Filtros con metadata (ej: `genre == 'Horror'`)
- Retrievers como abstracci√≥n sobre vector stores

### Parte 2: RAG Chain B√°sico
- Configurar LLM (ChatGroq)
- Obtener prompt de LangSmith Hub
- Crear chain con LCEL (LangChain Expression Language)
- Formato: `{"context": retriever, "question": input} | prompt | llm | parser`

**Ejemplo de query:**
```python
query = "I want to get a movie about religion"
result = rag_chain.invoke(query)
# Retriever busca pel√≠culas relevantes ‚Üí LLM responde bas√°ndose en ellas
```

### Parte 3: RAG con PDF (Climate Change)
- Cargar PDF con `PyPDFLoader`
- Chunking con `RecursiveCharacterTextSplitter`
- Crear ChromaDB con 97 chunks
- Retriever con k=5 documentos
- Pretty print de documentos recuperados

**Resultado:** Sistema que responde preguntas sobre cambio clim√°tico usando el PDF

### Parte 4: Re-ranking
- Implementaci√≥n manual con `CrossEncoder`
- Funci√≥n `rerank_documents()` que:
  1. Obtiene top K docs del retriever (ej: k=5)
  2. Re-rankea con cross-encoder
  3. Retorna top N mejores (ej: n=3)
- Clase `RerankedRetriever` para integrar en chains

**Mejora observable:** Documentos m√°s relevantes en top positions

### Parte 5: RAGxplorer (Visualizaci√≥n)
- Carga PDF y crea vector database
- Reduce dimensionalidad con UMAP (384D ‚Üí 2D)
- Visualiza chunks y query en espacio 2D
- Marca documentos recuperados en verde

**Utilidad:** Ver visualmente qu√© chunks est√°n cerca de la query

---

## üéì Aprendizajes Clave

### 1. **RAG es m√°s √∫til que fine-tuning para datos espec√≠ficos**
- No requiere re-entrenar el modelo
- F√°cil actualizar informaci√≥n (agregar/quitar docs)
- El LLM puede citar fuentes espec√≠ficas

### 2. **Pipeline t√≠pico de RAG**
```python
# 1. Cargar documentos
docs = PyPDFLoader("file.pdf").load()

# 2. Chunking
chunks = RecursiveCharacterTextSplitter(chunk_size=1000).split_documents(docs)

# 3. Crear vector store
vectorstore = Chroma.from_documents(chunks, embeddings)

# 4. Retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# 5. RAG Chain
chain = {"context": retriever, "question": RunnablePassthrough()} | prompt | llm | parser
```

### 3. **Trade-offs importantes**
- **Chunk size:** Peque√±o = preciso pero fragmentado, Grande = contexto completo pero menos espec√≠fico
- **Top K:** M√°s docs = m√°s contexto pero m√°s ruido
- **Re-ranking:** Mejora calidad pero agrega latencia

### 4. **LCEL (LangChain Expression Language)**
Sintaxis con pipe `|` para encadenar componentes:
```python
chain = step1 | step2 | step3
result = chain.invoke(input)
```

Equivalente a:
```python
temp1 = step1(input)
temp2 = step2(temp1)
result = step3(temp2)
```

### 5. **Metadata es poderosa**
Permite filtros avanzados:
```python
# Solo pel√≠culas de horror en 2023
retriever.invoke(query, filter={
    "genre": "Horror",
    "release_date": {"$gte": "2023-01-01"}
})
```

---

## üîç Comparaci√≥n: Sin Re-ranking vs Con Re-ranking

### Sin Re-ranking (solo embeddings)
```
Query: "What is the main cause of climate change?"

Top 3 documentos:
1. ...Holocene epoch...greenhouse gases...
2. ...agricultural sector's carbon footprint...
3. ...Understanding Climate Change Chapter 1...
```

### Con Re-ranking (cross-encoder)
```
Query: "What is the main cause of climate change?"

Re-ranking scores:
1. Score:  5.4590 - ...greenhouse gases...primary cause...
2. Score:  1.3634 - ...Introduction to Climate Change...
3. Score: -0.4736 - ...agricultural sector...

‚úÖ Documentos re-ordenados por relevancia sem√°ntica m√°s precisa
```

**Observaci√≥n:** El cross-encoder detecta que el primer documento es M√ÅS relevante porque menciona directamente "primary cause" y "greenhouse gases".

---

## üìä M√©tricas del Notebook

- **PDF:** Understanding_Climate_Change.pdf (33 p√°ginas, 206KB)
- **Chunks generados:** 97
- **Embedding model:** all-MiniLM-L6-v2 (384 dimensiones, ~90MB)
- **LLM:** llama-3.1-8b-instant (Groq, gratis)
- **Cross-encoder:** ms-marco-MiniLM-L-6-v2 (~80MB)
- **Dataset movies:** 10 pel√≠culas con metadata

---

## ‚úÖ Checklist de Ejecuci√≥n

- [x] Instalar dependencias (pypdf, langsmith, ragexplorer)
- [x] Adaptar imports para LangChain 1.0+
- [x] Configurar embeddings locales (HuggingFace)
- [x] Configurar LLM gratuito (Groq)
- [x] Copiar PDF a `notebooks/data/`
- [x] Ejecutar b√∫squeda sem√°ntica con movies
- [x] Crear RAG chain b√°sico
- [x] Cargar y procesar PDF (chunking)
- [x] Implementar re-ranking manual
- [x] Configurar RAGxplorer
- [x] Probar visualizaciones
- [x] Documentar en `raglangchain.md`

---

## üìù Notas Finales

### Diferencias con chatmodel.ipynb
- **chatmodel:** Prompting, chains, memoria, few-shot
- **raglangchain:** Vector search + LLM, retrieval, re-ranking

### Diferencias con semanticsearchnotebook.ipynb
- **semanticsearch:** Solo b√∫squeda vectorial (embeddings + ChromaDB)
- **raglangchain:** B√∫squeda vectorial + LLM para responder

### Pr√≥ximos pasos recomendados
1. **Ejecutar celda por celda** y observar resultados
2. **Experimentar con chunk_size** (500, 1000, 2000)
3. **Probar diferentes queries** en el PDF
4. **Comparar con/sin re-ranking** en tus propios datos
5. **Continuar con:** `react-web-search.ipynb` (agentes con b√∫squeda web)

---

## üéØ Conceptos para Recordar

| Concepto | Descripci√≥n | Ejemplo |
|----------|-------------|---------|
| **RAG** | Retrieval + Generation | Buscar docs ‚Üí LLM responde |
| **Embeddings** | Vectores sem√°nticos | "perro" ‚âà "can" |
| **Chunking** | Dividir docs en fragmentos | PDF ‚Üí 97 chunks de 1000 chars |
| **Retriever** | Busca top K docs | k=5 documentos m√°s similares |
| **Re-ranking** | Mejora orden de docs | Cross-encoder scores |
| **Vector Store** | DB de embeddings | ChromaDB, FAISS, Pinecone |
| **LCEL** | Chain con pipes | `step1 \| step2 \| step3` |

---

**üéì Conclusi√≥n:**
RAG es una t√©cnica fundamental para crear LLMs que usen informaci√≥n espec√≠fica de tu dominio sin fine-tuning. Este notebook cubre todo el pipeline: desde cargar documentos hasta generar respuestas con re-ranking y visualizaci√≥n.

**Progreso:** 3/8 notebooks completados (37.5%) ‚úÖ
